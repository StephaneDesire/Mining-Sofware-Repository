"""
03_metrics_rq1.py
-----------------
Calcul des métriques RQ1 : durée de review et taux de merge

RQ1: Are pull requests generated by AI agents that are reviewed by automated bots
accepted or rejected more quickly than those reviewed exclusively by human developers?

Métriques:
- Median review duration (time between pull request creation and closure)
- Pull request acceptance rate
- Effect sizes using non-parametric statistical tests (Mann-Whitney U test and Cliff's delta)
"""

import os
import pandas as pd
import numpy as np
from scipy import stats
from cliffs_delta import cliffs_delta


# Paramètres généraux

FINAL_DIR = "../data/final"
RESULTS_DIR = "../results/tables"

os.makedirs(RESULTS_DIR, exist_ok=True)


# 1. Chargement des données

print("Chargement des données pour RQ1...")

rq1_data = pd.read_parquet(f"{FINAL_DIR}/rq1_data.parquet")

print(f"Nombre de PRs analysées : {len(rq1_data)}")


# 2. Séparation des groupes

print("Séparation des groupes (RQ1) : PRs AI reviewées par bots vs par humains...")

# On répond à RQ1 : comparer les PRs générées par AI et reviewées par des bots
# vs celles reviewées par des humains. Le préprocessing doit avoir ajouté la
# colonne 'reviewer_type' (bot / human / none).
ai_prs = rq1_data[rq1_data["author_type"] == "ai"].copy()

# PRs AI reviewées par des bots
ai_by_bot = ai_prs[ai_prs["reviewer_type"] == "bot"].copy()

# PRs AI reviewées par des humains
ai_by_human = ai_prs[ai_prs["reviewer_type"] == "human"].copy()

print(f"PRs AI (total) : {len(ai_prs)}")
print(f"PRs AI reviewées par bots : {len(ai_by_bot)}")
print(f"PRs AI reviewées par humains : {len(ai_by_human)}")


# 3. Calcul des métriques descriptives

print("Calcul des métriques descriptives...")

results = []

# 3.1. Durée de review (médiane)
# Durée de review (médiane) pour PRs AI reviewées par bots vs humains
bot_duration = ai_by_bot["review_duration_hours"].dropna()
human_review_duration = ai_by_human["review_duration_hours"].dropna()

results.append({
    "metric": "review_duration_hours",
    "group": "AI_by_bot",
    "median": bot_duration.median(),
    "mean": bot_duration.mean(),
    "std": bot_duration.std(),
    "q25": bot_duration.quantile(0.25) if len(bot_duration)>0 else np.nan,
    "q75": bot_duration.quantile(0.75) if len(bot_duration)>0 else np.nan,
    "count": len(bot_duration)
})

results.append({
    "metric": "review_duration_hours",
    "group": "AI_by_human",
    "median": human_review_duration.median(),
    "mean": human_review_duration.mean(),
    "std": human_review_duration.std(),
    "q25": human_review_duration.quantile(0.25) if len(human_review_duration)>0 else np.nan,
    "q75": human_review_duration.quantile(0.75) if len(human_review_duration)>0 else np.nan,
    "count": len(human_review_duration)
})

# 3.2. Taux d'acceptation (merge rate)
ai_by_bot_merge = ai_by_bot["merged"].mean()
ai_by_human_merge = ai_by_human["merged"].mean()

results.append({
    "metric": "merge_rate",
    "group": "AI_by_bot",
    "value": ai_by_bot_merge,
    "count": len(ai_by_bot),
    "merged": ai_by_bot["merged"].sum()
})

results.append({
    "metric": "merge_rate",
    "group": "AI_by_human",
    "value": ai_by_human_merge,
    "count": len(ai_by_human),
    "merged": ai_by_human["merged"].sum()
})

# 3.3. Nombre de commentaires
ai_comments = ai_prs["n_comments"].dropna()
ai_by_bot_comments = ai_by_bot["n_comments"].dropna()
ai_by_human_comments = ai_by_human["n_comments"].dropna()

results.append({
    "metric": "n_comments",
    "group": "AI_by_bot",
    "median": ai_by_bot_comments.median() if len(ai_by_bot_comments)>0 else np.nan,
    "mean": ai_by_bot_comments.mean() if len(ai_by_bot_comments)>0 else np.nan,
    "std": ai_by_bot_comments.std() if len(ai_by_bot_comments)>0 else np.nan,
    "count": len(ai_by_bot_comments)
})

results.append({
    "metric": "n_comments",
    "group": "AI_by_human",
    "median": ai_by_human_comments.median() if len(ai_by_human_comments)>0 else np.nan,
    "mean": ai_by_human_comments.mean() if len(ai_by_human_comments)>0 else np.nan,
    "std": ai_by_human_comments.std() if len(ai_by_human_comments)>0 else np.nan,
    "count": len(ai_by_human_comments)
})


# 4. Tests statistiques

print("Calcul des tests statistiques...")

# 4.1. Test de Mann-Whitney U pour la durée de review (AI_by_bot vs AI_by_human)
if len(bot_duration) > 0 and len(human_review_duration) > 0:
    u_stat, p_value_duration = stats.mannwhitneyu(
        bot_duration,
        human_review_duration,
        alternative='two-sided'
    )
    # Calcul de l'effect size avec Cliff's delta
    d, d_interpretation = cliffs_delta(bot_duration, human_review_duration)

    results.append({
        "metric": "review_duration_hours",
        "test": "Mann-Whitney U",
        "statistic": u_stat,
        "p_value": p_value_duration,
        "effect_size": d,
        "effect_interpretation": d_interpretation
    })

# 4.2. Test du chi² pour le taux de merge
# Table de contingence : merged vs not merged pour AI vs Human
# 4.2. Test du chi² pour le taux de merge entre AI_by_bot et AI_by_human
contingency_table = pd.crosstab(
    rq1_data[ rq1_data["author_type"] == "ai"]["reviewer_type"],
    rq1_data[ rq1_data["author_type"] == "ai"]["merged"]
)

if contingency_table.shape == (2,2):
    chi2, p_value_merge, dof, expected = stats.chi2_contingency(contingency_table)
    results.append({
        "metric": "merge_rate",
        "test": "Chi-square",
        "statistic": chi2,
        "p_value": p_value_merge,
        "degrees_of_freedom": dof
    })
else:
    # Si la table n'a pas la forme 2x2 (p.ex. manque d'observations), on écrit N/A
    results.append({
        "metric": "merge_rate",
        "test": "Chi-square",
        "statistic": "N/A",
        "p_value": "N/A",
        "degrees_of_freedom": "N/A"
    })

# 4.3. Test de Mann-Whitney U pour le nombre de commentaires
if len(ai_by_bot_comments) > 0 and len(ai_by_human_comments) > 0:
    u_stat_comments, p_value_comments = stats.mannwhitneyu(
        ai_by_bot_comments,
        ai_by_human_comments,
        alternative='two-sided'
    )
    d_comments, d_comments_interpretation = cliffs_delta(ai_by_bot_comments, ai_by_human_comments)
    results.append({
        "metric": "n_comments",
        "test": "Mann-Whitney U",
        "statistic": u_stat_comments,
        "p_value": p_value_comments,
        "effect_size": d_comments,
        "effect_interpretation": d_comments_interpretation
    })


# 5. Sauvegarde des résultats

print("Sauvegarde des résultats...")

# Conversion en DataFrame pour faciliter l'export
results_df = pd.DataFrame(results)

# Sauvegarde en CSV
results_df.to_csv(f"{RESULTS_DIR}/rq1_metrics.csv", index=False)

# Sauvegarde d'un résumé formaté
summary = {
    "Metric": [],
    "AI_Median": [],
    "Human_Median": [],
    "AI_Mean": [],
    "Human_Mean": [],
    "P_Value": [],
    "Effect_Size": []
}

# Durée de review (AI by bot vs AI by human)
summary["Metric"].append("Review Duration (hours)")
summary["AI_Median"].append(f"{bot_duration.median():.2f}" if len(bot_duration)>0 else "N/A")
summary["Human_Median"].append(f"{human_review_duration.median():.2f}" if len(human_review_duration)>0 else "N/A")
summary["AI_Mean"].append(f"{bot_duration.mean():.2f}" if len(bot_duration)>0 else "N/A")
summary["Human_Mean"].append(f"{human_review_duration.mean():.2f}" if len(human_review_duration)>0 else "N/A")
summary["P_Value"].append(f"{p_value_duration:.4f}" if 'p_value_duration' in locals() else "N/A")
summary["Effect_Size"].append(f"{d:.3f} ({d_interpretation})" if 'd' in locals() else "N/A")

# Taux de merge (AI by bot vs AI by human)
summary["Metric"].append("Merge Rate")
summary["AI_Median"].append(f"{ai_by_bot_merge:.2%}" if not np.isnan(ai_by_bot_merge) else "N/A")
summary["Human_Median"].append(f"{ai_by_human_merge:.2%}" if not np.isnan(ai_by_human_merge) else "N/A")
summary["AI_Mean"].append("N/A")
summary["Human_Mean"].append("N/A")
summary["P_Value"].append(f"{p_value_merge:.4f}" if 'p_value_merge' in locals() else "N/A")
summary["Effect_Size"].append("N/A")

# Nombre de commentaires
summary["Metric"].append("Number of Comments")
summary["AI_Median"].append(f"{ai_by_bot_comments.median():.2f}" if len(ai_by_bot_comments)>0 else "N/A")
summary["Human_Median"].append(f"{ai_by_human_comments.median():.2f}" if len(ai_by_human_comments)>0 else "N/A")
summary["AI_Mean"].append(f"{ai_by_bot_comments.mean():.2f}" if len(ai_by_bot_comments)>0 else "N/A")
summary["Human_Mean"].append(f"{ai_by_human_comments.mean():.2f}" if len(ai_by_human_comments)>0 else "N/A")
summary["P_Value"].append(f"{p_value_comments:.4f}" if 'p_value_comments' in locals() else "N/A")
summary["Effect_Size"].append(f"{d_comments:.3f} ({d_comments_interpretation})" if 'd_comments' in locals() else "N/A")

summary_df = pd.DataFrame(summary)
summary_df.to_csv(f"{RESULTS_DIR}/rq1_summary.csv", index=False)

print("Métriques RQ1 calculées avec succès.")
print("\nRésumé RQ1:")
print(summary_df.to_string(index=False))

# Interprétation des Résultats et Conclusions
## Analyse AIDev (MSR Challenge)

  
**Dataset** : AIDev (repositories avec ≥ 500 stars)  
**Nombre total de PRs analysées** : 11,071 PRs (métriques finales pour RQ1–RQ3)

---

## Résumé Exécutif

Cette analyse examine trois questions de recherche (RQ) concernant les pull requests générées par des agents AI et leur processus de review. Les résultats révèlent des différences significatives entre les PRs en "closed-loop" (même provider pour l'agent et le bot de review) et "open-loop" (providers différents), ainsi que des insights sur les types de commentaires générés lors des reviews.

**Principales conclusions** :
- **30.7%** des PRs AI sont en closed-loop
- Les PRs closed-loop ont une **durée de review significativement plus longue** (médiane : 9.9h vs 2.8h)
- Le **taux d'acceptation est significativement plus faible** en closed-loop (53.7% vs 64.7%)
- Les commentaires sont principalement de type **corrective** (47.5%) et **testing** (40.6%)

---



## RQ1 : Durée de review et taux d'acceptation
### Question de recherche
*Are pull requests generated by AI agents that are reviewed by automated bots accepted or rejected more quickly than those reviewed by human reviewers?*

### Résultats observés (PRs AI : bots vs humans)

Les métriques suivantes comparent les PRs générées par des agents AI selon le type de reviewer détecté (`reviewer_type` déterminé à partir des reviews et commentaires) :
- **Groupe** : AI_by_bot (PRs AI reviewées par des bots) vs AI_by_human (PRs AI reviewées par des humains)
- **Nombre de PRs (AI)** : 11,071 au total
- **PRs AI reviewées par bots** : 4,558
- **PRs AI reviewées par humains** : 3,080

Note : les comparaisons ci‑dessus (AI_by_bot vs AI_by_human) portent uniquement sur le sous‑ensemble de PRs pour lesquelles un `reviewer_type` a pu être identifié (4,558 PRs reviewées par des bots + 3,080 PRs reviewées par des humains = 7,638 PRs). Les PRs restantes n'avaient pas de `reviewer_type` assigné et sont exclues de ces comparaisons.

Métriques principales :
- **Durée médiane de review** : **18.29 h** (AI_by_bot) vs **3.56 h** (AI_by_human)
- **Durée moyenne de review** : 102.27 h vs 86.35 h
- **Taux d'acceptation (merge rate)** : **56.12%** (AI_by_bot) vs **62.31%** (AI_by_human)
- **Nombre médian de commentaires** : **3.00** (AI_by_bot) vs **1.00** (AI_by_human)
- **Nombre moyen de commentaires** : 3.99 vs 2.06

Tests statistiques :
- Durée de review : Mann-Whitney U, p < 0.0001, Cliff's delta = 0.187 (small)
- Nombre de commentaires : Mann-Whitney U, p < 0.0001, Cliff's delta = 0.515 (large)
- Taux de merge : Chi-square (p-value fournie dans les résultats)

### Interprétation

Les PRs AI reviewées par des bots présentent une **durée médiane de review nettement plus élevée** (≈18.3 h) que celles reviewées par des humains (≈3.6 h). Elles reçoivent également **plus de commentaires** (médiane 3 vs 1) et ont un **taux de merge plus faible** (~56% vs ~62%).

Les tests non-paramétriques montrent que ces différences sont statistiquement significatives ; en particulier, l'ampleur de la différence du nombre de commentaires est large (Cliff's delta ≈ 0.515), ce qui suggère un effet substantiel.

Ces résultats indiquent que, pour les PRs AI, être reviewé par un bot s'accompagne en moyenne de reviews plus longues, plus de feedbacks et une probabilité de merge réduite comparée aux reviews humaines. Plusieurs interprétations sont possibles :
- Les bots détectent davantage de problèmes (ou appliquent des règles plus strictes), générant plus de commentaires et de cycles d'itération.
- Les workflows bots peuvent déclencher des vérifications automatisées (tests, linters) qui prolongent le temps jusqu'à la fermeture.

### Conclusion RQ1

Oui — en condition observée, les PRs AI reviewées par des bots sont traitées plus longuement, reçoivent plus de commentaires et ont un taux d'acceptation inférieur par rapport aux PRs AI reviewées par des humains. Cela répond directement à RQ1 en mettant en évidence l'impact du type de reviewer sur les résultats des PRs générées par AI.



## RQ2 : Types de commentaires de review
### Question de recherche
*What types of review comments (e.g., corrective, stylistic, security-related, or testing-related) are most frequently produced by automated review bots on AI-generated pull requests, and how are these comments addressed by human developers?*

### Résultats observés

**Volume de données** :
- **Total de commentaires analysés** : 24,559 commentaires
- **PRs avec commentaires** : 7,638 PRs uniques
- **Moyenne de commentaires par PR** : 3.22 commentaires
- **Taux d'acceptation des PRs avec commentaires** : 58.6%

**Distribution par catégorie** :

| Catégorie | Nombre | Pourcentage | Sentiment dominant |
|-----------|--------|-------------|-------------------|
| **Corrective** | 11,659 | **47.5%** | Neutre (49%), Négatif (46%) |
| **Testing** | 9,964 | **40.6%** | Neutre (58%), Négatif (35%) |
| **Other** | 7,243 | 29.5% | Neutre (90%) |
| **Security** | 3,632 | 14.8% | Négatif (59%) |
| **Style** | 3,560 | 14.5% | Négatif (55%) |

*Note : Un commentaire peut appartenir à plusieurs catégories, d'où un total > 100%*

**Analyse du sentiment** :
- **Commentaires négatifs** : Principalement dans les catégories Security (59%) et Style (55%)
- **Commentaires neutres** : Dominants dans Corrective (49%) et Testing (58%)
- **Commentaires positifs** : Minoritaires dans toutes les catégories (< 10%)

### Interprétation

**1. Prédominance des commentaires correctifs et de testing**

Les commentaires **correctifs** (47.5%) et **testing** (40.6%) dominent largement, ce qui suggère que :
- Les bots de review détectent principalement des **erreurs fonctionnelles** et des **problèmes de logique**
- Les PRs AI nécessitent souvent des **améliorations de tests** ou des tests manquants
- Les reviewers (bots ou humains) se concentrent sur la **correction fonctionnelle** plutôt que sur le style

**2. Ton des commentaires**

Le fait que les commentaires soient majoritairement **neutres** ou **négatifs** (peu de positifs) indique :
- Un processus de review **objectif et factuel**
- Une focalisation sur les **problèmes à corriger** plutôt que sur les aspects positifs
- Les commentaires de sécurité et de style ont un ton plus négatif, reflétant probablement des préoccupations légitimes

**3. Taux d'acceptation après commentaires**

Le taux d'acceptation de 58.6% pour les PRs avec commentaires suggère que :
- La majorité des PRs sont **corrigées et acceptées** après avoir reçu des commentaires
- Les commentaires sont généralement **constructifs** et permettent d'améliorer la qualité
- Environ 40% des PRs sont soit fermées sans merge, soit nécessitent des itérations supplémentaires

### Conclusion RQ2

**Conclusion principale** : Les commentaires de review sur les PRs AI sont principalement de nature **corrective** et **testing**, avec un ton généralement neutre ou négatif mais constructif. Cela suggère que :

1. **Les bots de review sont efficaces** pour détecter les problèmes fonctionnels et les lacunes dans les tests
2. **Les développeurs répondent aux commentaires** : 58.6% des PRs avec commentaires sont finalement mergées
3. **Les préoccupations de sécurité et de style** sont moins fréquentes mais traitées avec un ton plus critique, ce qui est approprié

**Implications** :
- Les outils de génération de code AI devraient être améliorés pour réduire les erreurs fonctionnelles
- Une attention particulière devrait être portée à la génération de tests complets
- Les commentaires de review sont généralement constructifs et permettent d'améliorer la qualité du code

---

## RQ3 : Biais du closed-loop
### Question de recherche
*Does a "closed-loop" bias exist when the AI coding agent and the automated review bot originate from the same provider, and how does this affect pull request outcomes and review behavior?*

### Résultats observés

**Proportion de closed-loop** :
- **30.7%** des PRs AI sont en closed-loop (3,397 PRs)
- **69.3%** sont en open-loop (7,674 PRs)

**Comparaison closed-loop vs open-loop** :

| Métrique | Closed-Loop | Open-Loop | Différence | P-value | Effect Size |
|----------|-------------|-----------|------------|---------|-------------|
| **Durée médiane de review** | 9.90h | 2.83h | **+7.07h** | < 0.0001 | Small (0.181) |
| **Durée moyenne de review** | 88.21h | 74.46h | +13.75h | < 0.0001 | - |
| **Taux d'acceptation** | **53.7%** | **64.7%** | **-11.0%** | < 0.0001 | - |
| **Nombre médian de commentaires** | 3.0 | 1.0 | **+2.0** | < 0.0001 | Large (0.587) |
| **Nombre moyen de commentaires** | 3.36 | 1.71 | +1.65 | < 0.0001 | - |

**Tests statistiques** :
- **Durée de review** : Mann-Whitney U test, p < 0.0001, effect size = 0.181 (small)
- **Taux d'acceptation** : Chi-square test, p < 0.0001
- **Nombre de commentaires** : Mann-Whitney U test, p < 0.0001, effect size = 0.587 (large)

### Interprétation

**1. Existence d'un biais closed-loop : CONFIRMÉ**

Les résultats montrent clairement l'existence d'un biais closed-loop avec des différences statistiquement significatives dans plusieurs dimensions :

**2. Durée de review plus longue en closed-loop**

- La durée médiane est **3.5 fois plus longue** en closed-loop (9.9h vs 2.8h)
- Cette différence est statistiquement significative (p < 0.0001)
- **Interprétation** : Les PRs closed-loop nécessitent plus de temps de review, suggérant soit :
  - Des standards de qualité plus stricts lorsque le même provider contrôle les deux étapes
  - Des processus de review plus approfondis
  - Des itérations supplémentaires nécessaires

**3. Taux d'acceptation significativement plus faible**

- Le taux d'acceptation est **11 points de pourcentage plus faible** en closed-loop (53.7% vs 64.7%)
- Cette différence est hautement significative (p < 0.0001)
- **Interprétation** : Contrairement à l'hypothèse d'un biais positif (acceptation plus facile), les PRs closed-loop sont **moins acceptées**. Cela suggère :
  - Des critères de qualité **plus stricts** en closed-loop
  - Une volonté du provider de maintenir une réputation de qualité élevée
  - Possiblement une détection plus efficace des problèmes par les bots du même provider

**4. Nombre de commentaires significativement plus élevé**

- Le nombre médian de commentaires est **3 fois plus élevé** en closed-loop (3.0 vs 1.0)
- L'effect size est **large** (0.587), indiquant une différence substantielle
- **Interprétation** : Les PRs closed-loop génèrent **beaucoup plus de commentaires**, ce qui indique :
  - Des reviews **plus approfondies** et détaillées
  - Une détection plus fine des problèmes
  - Possiblement des standards de code plus élevés

**5. Implications du biais**

Le biais closed-loop observé est **contraire à l'hypothèse initiale** d'un biais positif (favorisant l'acceptation). Au contraire :

- **Biais négatif observé** : Les PRs closed-loop sont traitées avec **plus de rigueur**
- **Qualité potentiellement meilleure** : Les standards plus stricts pourraient mener à du code de meilleure qualité
- **Transparence** : Le fait que le même provider contrôle les deux étapes ne semble pas créer un conflit d'intérêt favorisant l'acceptation

### Conclusion RQ3

**Conclusion principale** : **Un biais closed-loop existe bel et bien**, mais il est de nature **négative** plutôt que positive. Les PRs où l'agent AI et le bot de review proviennent du même provider sont :

1. **Reviewées plus longuement** (durée médiane 3.5x plus longue)
2. **Moins acceptées** (taux d'acceptation 11% plus faible)
3. **Soumises à plus de commentaires** (3x plus de commentaires en médiane)

**Interprétation du biais** :

Ce biais "négatif" peut être interprété de deux façons :

**Hypothèse 1 : Standards de qualité plus élevés**
- Les providers appliquent des critères plus stricts en closed-loop pour maintenir leur réputation
- La connaissance approfondie du système par le même provider permet une détection plus fine des problèmes
- Résultat : Code de meilleure qualité mais processus plus long

**Hypothèse 2 : Conflit d'intérêt inversé**
- Les providers pourraient être plus critiques pour éviter l'apparence de favoritisme
- Une volonté de démontrer l'objectivité du processus de review
- Résultat : Rejets plus fréquents même pour des PRs de qualité acceptable

**Implications pratiques** :

1. **Pour les développeurs** : S'attendre à des reviews plus longues et plus strictes en closed-loop
2. **Pour les providers** : Le biais négatif observé suggère que les processus closed-loop ne favorisent pas l'acceptation, ce qui est positif pour la crédibilité
3. **Pour la recherche** : Des études supplémentaires sont nécessaires pour comprendre les mécanismes sous-jacents de ce biais

**Recommandations** :

- **Transparence** : Les providers devraient documenter leurs processus de review en closed-loop
- **Équité** : Des mécanismes devraient être mis en place pour garantir que les PRs closed-loop et open-loop soient traitées équitablement
- **Recherche future** : Analyser la qualité finale du code mergé pour déterminer si les standards plus stricts en closed-loop mènent effectivement à une meilleure qualité

---

## Conclusions Générales

### Synthèse des trois RQ

**RQ1** : Impact du type de reviewer pour les PRs AI (bots vs humains). Les résultats montrent que, parmi les PRs générées par des agents AI, celles reviewées par des bots ont une durée médiane de review plus élevée (≈18.3 h vs ≈3.6 h pour les PRs reviewées par des humains), reçoivent plus de commentaires (médiane 3 vs 1) et ont un taux d'acceptation plus faible (≈56.1% vs ≈62.3%). Ces différences sont statistiquement significatives et correspondent directement à la question RQ1 (effet du type de reviewer).

**RQ2** : Les commentaires de review sont principalement correctifs (47.5%) et liés aux tests (40.6%), avec un ton généralement neutre ou négatif mais constructif. 58.6% des PRs avec commentaires sont finalement mergées.

**RQ3** : **Biais closed-loop confirmé** mais de nature négative : les PRs closed-loop sont reviewées plus longuement, moins acceptées, et génèrent plus de commentaires. Cela suggère des standards de qualité plus élevés plutôt qu'un favoritisme.

### Implications pour la recherche et la pratique

1. **Qualité du code AI** : Les résultats suggèrent que les outils de génération de code AI nécessitent des améliorations, particulièrement dans la génération de tests et la réduction d'erreurs fonctionnelles.

2. **Processus de review** : Les bots de review sont efficaces pour détecter les problèmes, mais les processus closed-loop semblent appliquer des standards plus stricts.

3. **Transparence et équité** : Les différences observées entre closed-loop et open-loop justifient une transparence accrue sur les processus de review.

4. **Recherche future** : Des études supplémentaires sont nécessaires pour :
   - Comparer la qualité finale du code mergé en closed-loop vs open-loop
   - Analyser les mécanismes sous-jacents du biais closed-loop
   - Inclure des PRs humaines dans l'analyse pour RQ1

---

## Limitations de l'étude

1. **Couverture des PRs humaines** : L'extraction et le filtrage peuvent réduire la couverture des PRs humaines ou les rendre non-comparables (jointures sur URL, différences de schéma). Vérifier la proportion de PRs humaines incluses et la méthode d'appariement avant d'interpréter des comparaisons AI vs Human.

2. **Catégorisation des commentaires** : La catégorisation basée sur des mots-clés est simplifiée et pourrait bénéficier de techniques NLP plus avancées.

3. **Détection du closed-loop** : La détection du closed-loop basée sur les mots-clés dans le login de l'auteur est approximative et pourrait être améliorée.

4. **Représentativité** : L'analyse se limite aux repositories avec ≥ 500 stars, ce qui pourrait ne pas être représentatif de l'ensemble des projets.

---

## Références — Figures et tables

Figures principales (dans `results/figures/`) :
- `comparative_analysis.png` — analyse comparative générale
- `rq1_review_duration_and_merge_rate.png` — distribution des durées de review et taux de merge (RQ1)
- `rq1_comments_distribution.png` — distribution des commentaires (RQ1)
- `rq2_category_distribution.png` — répartition des catégories de commentaires (RQ2)
- `rq2_sentiment_by_category.png` — sentiment par catégorie (RQ2)
- `rq3_closed_loop_comparison.png` — comparaison closed-loop vs open-loop (RQ3)
- `rq3_closed_loop_proportion.png` — proportion closed-loop vs open-loop (RQ3)

Tables de synthèse (dans `results/tables/`) :
- `rq1_summary.csv` — métriques RQ1 (médianes, moyennes, merge rates)
- `rq2_summary.csv` — métriques RQ2 (commentaires, sentiments)
- `rq3_summary.csv` — métriques RQ3 (closed-loop vs open-loop)




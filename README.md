# AIDev Analysis Pipeline
## MSR Challenge - Analyse des Pull Requests gÃ©nÃ©rÃ©es par des agents AI

[![CI Pipeline](https://github.com/YOUR_USERNAME/AIDev-Steph/actions/workflows/ci.yml/badge.svg)](https://github.com/YOUR_USERNAME/AIDev-Steph/actions/workflows/ci.yml)

Ce projet implÃ©mente un pipeline complet d'analyse du dataset AIDev pour rÃ©pondre Ã  trois questions de recherche (RQ) concernant les pull requests gÃ©nÃ©rÃ©es par des agents AI et leur processus de review.

## ğŸ“‹ Table des matiÃ¨res

- [Description](#description)
- [Questions de recherche](#questions-de-recherche)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [RÃ©sultats](#rÃ©sultats)
- [DÃ©pendances](#dÃ©pendances)
- [Contributeurs](#contributeurs)
- [License](#license)

## ğŸ¯ Description

Ce pipeline analyse le dataset AIDev (HuggingFace) pour Ã©tudier :
- Les diffÃ©rences entre les PRs gÃ©nÃ©rÃ©es par des agents AI et celles gÃ©nÃ©rÃ©es par des humains
- Les types de commentaires produits lors des reviews automatisÃ©es
- L'existence d'un biais "closed-loop" lorsque l'agent AI et le bot de review proviennent du mÃªme provider

Le dataset analysÃ© se concentre sur les repositories populaires (â‰¥ 500 stars) pour garantir une qualitÃ© minimale des projets Ã©tudiÃ©s.

## ğŸ”¬ Questions de recherche

### RQ1 : DurÃ©e de review et taux d'acceptation
*Are pull requests generated by AI agents that are reviewed by automated bots accepted or rejected more quickly than those reviewed exclusively by human developers?*

**MÃ©triques** :
- DurÃ©e mÃ©diane de review (temps entre crÃ©ation et fermeture)
- Taux d'acceptation des PRs
- Effect sizes avec tests statistiques non-paramÃ©triques (Mann-Whitney U, Cliff's delta)

### RQ2 : Types de commentaires de review
*What types of review comments (e.g., corrective, stylistic, security-related, or testing-related) are most frequently produced by automated review bots on AI-generated pull requests, and how are these comments addressed by human developers?*

**MÃ©triques** :
- CatÃ©gorisation des commentaires (corrective, style, security, testing)
- Taux de rÃ©solution (suivi des commits de suivi)
- Analyse de sentiment pour Ã©valuer le ton des commentaires

### RQ3 : Biais closed-loop
*Does a "closed-loop" bias exist when the AI coding agent and the automated review bot originate from the same provider, and how does this affect pull request outcomes and review behavior?*

**MÃ©triques** :
- Proportion de PRs avec agents et bots du mÃªme provider
- Comparaison des taux d'acceptation entre closed-loop et open-loop
- Comparaison des durÃ©es de review entre closed-loop et open-loop

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10 ou supÃ©rieur
- pip (gestionnaire de paquets Python)
- AccÃ¨s Ã  HuggingFace Hub (pour tÃ©lÃ©charger le dataset)

### Installation des dÃ©pendances

```bash
# Cloner le repository
git clone https://github.com/YOUR_USERNAME/AIDev-Steph.git
cd AIDev-Steph

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Configuration

Le pipeline tÃ©lÃ©charge automatiquement les donnÃ©es depuis HuggingFace. Aucune configuration supplÃ©mentaire n'est nÃ©cessaire pour une utilisation standard.

## ğŸ’» Utilisation

### ExÃ©cution complÃ¨te du pipeline

Pour exÃ©cuter l'ensemble du pipeline d'analyse :

```bash
bash run_pipeline.sh
```

Le script exÃ©cute automatiquement toutes les Ã©tapes :
1. Chargement et filtrage des donnÃ©es
2. PrÃ©processing
3. Calcul des mÃ©triques RQ1, RQ2, RQ3
4. GÃ©nÃ©ration des visualisations

### ExÃ©cution Ã©tape par Ã©tape

Vous pouvez Ã©galement exÃ©cuter chaque script individuellement :

```bash
cd src

# Ã‰tape 1 : Chargement et filtrage
python3 01_load_filter.py

# Ã‰tape 2 : PrÃ©processing
python3 02_preprocess.py

# Ã‰tape 3 : MÃ©triques RQ1
python3 03_metrics_rq1.py

# Ã‰tape 4 : MÃ©triques RQ2
python3 04_metrics_rq2.py

# Ã‰tape 5 : MÃ©triques RQ3
python3 05_metrics_rq3.py

# Ã‰tape 6 : Visualisations
python3 06_visualizations.py
```

## ğŸ“ Structure du projet

```
AIDev-Steph/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # Configuration CI/CD
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes (tÃ©lÃ©chargÃ©es depuis HuggingFace)
â”‚   â”œâ”€â”€ intermediate/           # DonnÃ©es intermÃ©diaires (aprÃ¨s filtrage)
â”‚   â””â”€â”€ final/                 # DonnÃ©es finales (aprÃ¨s prÃ©processing)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ tables/                # Tables de rÃ©sultats (CSV)
â”‚   â”œâ”€â”€ figures/               # Visualisations (PNG)
â”‚   â””â”€â”€ INTERPRETATION_ET_CONCLUSIONS.md  # InterprÃ©tation des rÃ©sultats
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01_load_filter.py      # Chargement et filtrage des donnÃ©es
â”‚   â”œâ”€â”€ 02_preprocess.py       # PrÃ©processing des donnÃ©es
â”‚   â”œâ”€â”€ 03_metrics_rq1.py      # Calcul des mÃ©triques RQ1
â”‚   â”œâ”€â”€ 04_metrics_rq2.py      # Calcul des mÃ©triques RQ2
â”‚   â”œâ”€â”€ 05_metrics_rq3.py      # Calcul des mÃ©triques RQ3
â”‚   â””â”€â”€ 06_visualizations.py   # GÃ©nÃ©ration des visualisations
â”œâ”€â”€ run_pipeline.sh            # Script principal d'exÃ©cution
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸ“Š RÃ©sultats

### Fichiers gÃ©nÃ©rÃ©s

AprÃ¨s l'exÃ©cution du pipeline, les rÃ©sultats sont disponibles dans :

- **Tables** (`results/tables/`) :
  - `rq1_summary.csv` : RÃ©sumÃ© des mÃ©triques RQ1
  - `rq1_metrics.csv` : MÃ©triques dÃ©taillÃ©es RQ1
  - `rq2_summary.csv` : RÃ©sumÃ© des mÃ©triques RQ2
  - `rq2_category_stats.csv` : Statistiques par catÃ©gorie de commentaires
  - `rq2_sentiment_by_category.csv` : Distribution du sentiment par catÃ©gorie
  - `rq3_summary.csv` : RÃ©sumÃ© des mÃ©triques RQ3
  - `rq3_metrics.csv` : MÃ©triques dÃ©taillÃ©es RQ3

- **Figures** (`results/figures/`) :
  - `rq1_review_duration_and_merge_rate.png` : Comparaison durÃ©e et taux de merge (RQ1)
  - `rq1_comments_distribution.png` : Distribution du nombre de commentaires (RQ1)
  - `rq2_category_distribution.png` : Distribution des catÃ©gories de commentaires (RQ2)
  - `rq2_sentiment_by_category.png` : Sentiment par catÃ©gorie (RQ2)
  - `rq3_closed_loop_comparison.png` : Comparaison closed-loop vs open-loop (RQ3)
  - `rq3_closed_loop_proportion.png` : Proportion de closed-loop (RQ3)
  - `comparative_analysis.png` : Analyse comparative globale

- **InterprÃ©tation** (`results/INTERPRETATION_ET_CONCLUSIONS.md`) :
  - Analyse dÃ©taillÃ©e des rÃ©sultats
  - Conclusions pour chaque RQ
  - Implications et recommandations

### Principaux rÃ©sultats


#### RQ1
- Comparaison (PRs AI selon type de reviewer dÃ©tectÃ©)
- **DurÃ©e mÃ©diane de review** : 18.29 heures (PRs AI reviewÃ©es par bots) vs 3.56 heures (PRs AI reviewÃ©es par humains)
- **Taux d'acceptation** : 56.12% (AI_by_bot) vs 62.31% (AI_by_human)
- **Nombre mÃ©dian de commentaires** : 3 (AI_by_bot) vs 1 (AI_by_human)

#### RQ2
- **Commentaires les plus frÃ©quents** :
  - Corrective : 47.5%
  - Testing : 40.6%
  - Security : 14.8%
  - Style : 14.5%
- **Taux d'acceptation aprÃ¨s commentaires** : 58.6%

#### RQ3
- **Proportion de closed-loop** : 30.7%
- **DurÃ©e mÃ©diane closed-loop** : 9.90h vs 2.83h (open-loop)
- **Taux d'acceptation closed-loop** : 53.7% vs 64.7% (open-loop)
- **Nombre mÃ©dian de commentaires closed-loop** : 3.0 vs 1.0 (open-loop)

Pour plus de dÃ©tails, consultez `results/INTERPRETATION_ET_CONCLUSIONS.md`.

## ğŸ“¦ DÃ©pendances

Les principales dÃ©pendances sont listÃ©es dans `requirements.txt` :

- **pandas** : Manipulation de donnÃ©es
- **numpy** : Calculs numÃ©riques
- **scipy** : Tests statistiques
- **matplotlib** & **seaborn** : Visualisations
- **cliffs-delta** : Calcul des effect sizes
- **pyarrow** & **fastparquet** : Lecture/Ã©criture de fichiers Parquet
- **huggingface_hub** : AccÃ¨s au dataset HuggingFace

Pour une liste complÃ¨te, voir `requirements.txt`.

## ğŸ§ª Tests et CI/CD

Le projet inclut une configuration CI/CD avec GitHub Actions qui :

- VÃ©rifie la syntaxe Python de tous les scripts
- Teste l'importation des modules
- Valide la structure du projet
- ExÃ©cute des vÃ©rifications de linting (optionnel)

Pour exÃ©cuter les vÃ©rifications localement :

```bash
# VÃ©rification de la syntaxe
python -m py_compile src/*.py

# VÃ©rification des dÃ©pendances
python -c "import pandas, numpy, scipy, matplotlib, seaborn, cliffs_delta"
```

## ğŸ“ Notes importantes



### Limitations et points d'attention (version rÃ©visÃ©e)

Nous dÃ©taillons ci-dessous les limites mÃ©thodologiques identifiÃ©es pendant l'exÃ©cution du pipeline et les implications pour l'interprÃ©tation des rÃ©sultats.

1) ReprÃ©sentativitÃ© des donnÃ©es
- Le filtrage sur les repositories avec â‰¥ 500 stars rÃ©duit la population Ã©tudiÃ©e aux projets populaires ; les comportements observÃ©s (politique de review, prÃ©sence d'outils CI/bots, proportions closed/open-loop) peuvent diffÃ©rer dans des projets plus petits.

2) DÃ©tection du type de reviewer (heuristique)
- La colonne `reviewer_type` est dÃ©terminÃ©e par une heuristique qui recherche des logins/artifacts indiquant des bots dans les tables `pr_reviews` et `pr_comments`. Cette mÃ©thode est simple et efficace pour capter de nombreux bots connus, mais :
  - Beaucoup de PRs restent classÃ©es `none` (pas de reviewer identifiÃ©) â€” par ex. ~9,536 PRs dans le jeu nettoyÃ© â€” ce qui limite la couverture.
  - Les bots non identifiables par login (ou bots internes avec logins non-standard) peuvent Ãªtre manquÃ©s.
  - La prÃ©sence d'un login de bot dans les commentaires ne garantit pas que la review principale a Ã©tÃ© effectuÃ©e par le bot.

3) DÃ©tection du closed-loop
- Le flag `closed_loop` repose sur la prÃ©sence de mots-clÃ©s (par ex. "copilot", "claude") dans le login de l'auteur ; c'est une approximation qui peut gÃ©nÃ©rer faux positifs et faux nÃ©gatifs (providers avec noms diffÃ©rents, changements de login, organisations intermÃ©diaires).

4) CatÃ©gorisation des commentaires
- La classification des commentaires (corrective, testing, security, style, other) est basÃ©e sur des rÃ¨gles / mots-clÃ©s. Elle est sujette Ã  erreurs de couverture et d'ambiguÃ¯tÃ© ; des mÃ©thodes NLP supervisÃ©es ou semi-supervisÃ©es amÃ©lioreraient la prÃ©cision et la granularitÃ©.

5) Ã‰chantillonnage et biais d'extraction
- Certaines tables (p.ex. `human_pull_request`) doivent Ãªtre jointes via l'URL du dÃ©pÃ´t â€” des mismatches d'URL ou des entrÃ©es manquantes peuvent conduire Ã  des pertes d'observations. VÃ©rifier la proportion de PRs exclues lors du merge si l'on veut Ã©tendre la comparaison AI vs Human.

6) Effets temporels et dÃ©pendances externes
- Les durÃ©es de review peuvent Ãªtre affectÃ©es par la pÃ©riode (vacances, Ã©vÃ¨nements) et par des Ã©tapes externes (CI longs, jobs de tests). Le pipeline n'isole pas toujours ces facteurs.

7) Mesures d'impact limitÃ©es
- Les mÃ©triques calculÃ©es (durÃ©e, merge rate, nombre de commentaires) mesurent des corrÃ©lations. Elles n'Ã©tablissent pas de causalitÃ© (p.ex. que la prÃ©sence d'un bot cause un taux de merge plus faible).

Comment attÃ©nuer ces limites (recommandations)
- Ã‰chantillonnage : Ã©tendre l'analyse Ã  repositories < 500 stars pour tester la robustesse des rÃ©sultats.
- AmÃ©liorer la dÃ©tection des reviewers : utiliser les mÃ©tadonnÃ©es des events (actors, types de review), tokens fournis par `pr_reviews` (si disponibles) ou scraper les events GitHub pour rÃ©cupÃ©rer les types rÃ©els des acteurs.
- CatÃ©gorisation NLP : entraÃ®ner un classifieur supervisÃ© sur un Ã©chantillon annotÃ© pour amÃ©liorer la prÃ©cision des catÃ©gories de commentaires.
- Validation manuelle : effectuer un audit manuelle (sample stratifiÃ©) pour estimer le taux d'erreur des heuristiques (closed-loop, reviewer_type, catÃ©gories).
- ContrÃ´les additionnels : intÃ©grer indicateurs CI (durÃ©e des jobs), labels de PR et autres mÃ©tadonnÃ©es pour mieux contrÃ´ler les diffÃ©rences de procÃ©dure entre groupes.

Note : ces limitations sont normales pour une premiÃ¨re itÃ©ration d'un pipeline exploratoire ; elles sont documentÃ©es ici pour orienter les travaux futurs et la prudence dans l'interprÃ©tation.

### DonnÃ©es

Les donnÃ©es sont tÃ©lÃ©chargÃ©es automatiquement depuis HuggingFace lors de la premiÃ¨re exÃ©cution :
- Dataset : `hao-li/AIDev`
- Format : Parquet
- Taille approximative : ~500 MB (selon la connexion)

Les donnÃ©es brutes sont sauvegardÃ©es dans `data/raw/` pour garantir la reproductibilitÃ©.

## ğŸ¤ Contributeurs

- ZOUMBA SomyalaguÃ©do DÃ©sirÃ© StÃ©phane - Analyse et dÃ©veloppement

## ğŸ“„ License

Ce projet est fourni sous licence [MIT/autre] - voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ“š RÃ©fÃ©rences

- Dataset AIDev : [HuggingFace](https://huggingface.co/datasets/hao-li/AIDev)
- MSR Challenge : [Site officiel](https://msrchallenge.org/)

### ğŸ› ProblÃ¨mes connus

- Le filtrage initial peut rÃ©duire la reprÃ©sentativitÃ© des PRs humaines dans certains sous-ensembles ; vÃ©rifier l'Ã©chantillonnage lors d'analyses comparatives (RQ1).

## ğŸ”„ Mises Ã  jour futures

- [ ] AmÃ©lioration de la dÃ©tection du closed-loop
- [ ] Inclusion de PRs humaines pour RQ1
- [ ] CatÃ©gorisation NLP plus avancÃ©e pour RQ2
- [ ] Analyse de la qualitÃ© finale du code mergÃ©

---


